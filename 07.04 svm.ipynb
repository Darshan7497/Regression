{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52965bf4-013d-4b75-a5e0-f1966f060d53",
   "metadata": {},
   "source": [
    "# Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79d14cc-0ca0-4e8c-bbb5-3e9c43e4cc04",
   "metadata": {},
   "source": [
    "In machine learning algorithms, the relationship between polynomial functions and kernel functions is closely tied to their use in feature transformation and the creation of non-linear decision boundaries.\n",
    "\n",
    "Polynomial Functions:\n",
    "Polynomial functions are mathematical functions that involve powers of variables (e.g., x^2, x^3, etc.). In the context of machine learning, polynomial functions are used to create new features by raising the existing features to different powers. For example, if you have a 2-dimensional feature vector (x, y), you can create new features such as (x^2, y^2, x*y, etc.) using polynomial functions.\n",
    "The idea of using polynomial functions in machine learning is to transform the original feature space into a higher-dimensional feature space. By introducing these polynomial features, we can fit more complex relationships between the features and the target variable. However, using high-degree polynomial features can lead to overfitting, especially when the number of features becomes large.\n",
    "\n",
    "Kernel Functions:\n",
    "Kernel functions, on the other hand, are used in the context of kernel methods, such as Support Vector Machines (SVMs). Kernel functions provide a way to implicitly perform the feature transformation into a higher-dimensional space without explicitly calculating the new features. They allow us to operate in the original feature space while effectively obtaining the benefits of working in a higher-dimensional space.\n",
    "The choice of kernel function determines how the data points in the original feature space are mapped into the higher-dimensional feature space. Commonly used kernel functions include the linear kernel, polynomial kernel, radial basis function (RBF) kernel, and sigmoid kernel. The polynomial kernel is particularly interesting as it involves polynomial functions.\n",
    "\n",
    "The polynomial kernel can be defined as K(x, x') = (1 + x^T * x')^d, where 'd' is the degree of the polynomial. The polynomial kernel computes the dot product of the transformed feature vectors in the higher-dimensional space without explicitly calculating the polynomial features. This allows SVMs, for example, to create non-linear decision boundaries in the original feature space without actually operating in the higher-dimensional space.\n",
    "\n",
    "In summary, the relationship between polynomial functions and kernel functions lies in their role in creating non-linear decision boundaries. Polynomial functions are used to explicitly transform the original features into higher-dimensional space, while kernel functions enable us to implicitly operate in higher-dimensional space while remaining in the original feature space. The polynomial kernel, in particular, leverages the concept of polynomial functions to create non-linear decision boundaries in kernel methods like SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dfcc9c-69bc-4e5f-8790-0ec9a657da27",
   "metadata": {},
   "source": [
    "# ploynomial function and polynomial kernal are different .explain with simple example.\n",
    "\n",
    "\n",
    "\n",
    "polynomial functions and polynomial kernels are different concepts in machine learning, although they share some similarities. Let's explain each concept with simple examples and highlight their differences:\n",
    "\n",
    "Polynomial Function:\n",
    "A polynomial function is a mathematical function that involves powers of variables. In the context of machine learning, a polynomial function transforms the input features into higher-dimensional feature space by introducing polynomial terms. For example, a 2nd-degree polynomial function of a single variable x is given by:\n",
    "f(x) = a * x^2 + b * x + c\n",
    "\n",
    "In this case, the function f(x) involves a quadratic term (x^2), a linear term (x), and a constant term (c). By using polynomial functions, we can fit curved or nonlinear relationships between features and target variables. However, it explicitly calculates the new features.\n",
    "\n",
    "Polynomial Kernel:\n",
    "A polynomial kernel is a type of kernel function used in kernel methods, particularly in Support Vector Machines (SVM). Kernels are used to implicitly perform feature transformations into higher-dimensional spaces without explicitly calculating the new features. The polynomial kernel computes the dot product of data points in a higher-dimensional space.\n",
    "For example, the polynomial kernel of degree 2 for two variables x and y is given by:\n",
    "\n",
    "K(x, y) = (x^T * y + c)^2\n",
    "\n",
    "Here, x and y are the input feature vectors, and c is a constant term. The polynomial kernel allows SVMs to operate in a higher-dimensional feature space without explicitly computing the new feature vectors.\n",
    "\n",
    "Difference:\n",
    "The main difference between polynomial functions and polynomial kernels lies in how they handle feature transformations:\n",
    "\n",
    "Polynomial functions explicitly compute the new features in a higher-dimensional space and directly use these transformed features in the model. This can be computationally expensive, especially for high degrees of the polynomial.\n",
    "\n",
    "Polynomial kernels, on the other hand, implicitly operate in the higher-dimensional space without explicitly calculating the transformed features. Instead, they efficiently compute the dot product of the transformed feature vectors using kernel trick, which avoids the need to store or compute the transformed features explicitly.\n",
    "\n",
    "In summary, polynomial functions and polynomial kernels both involve polynomial terms, but polynomial functions explicitly calculate the transformed features, while polynomial kernels efficiently operate in the higher-dimensional space without explicitly computing the new feature vectors. Polynomial kernels are particularly useful in SVMs for handling nonlinear relationships between features and target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78272395-8d70-44d1-b0ad-bba642869167",
   "metadata": {},
   "source": [
    "# Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f7cff-4b8d-4a4b-8618-6f49063a6e93",
   "metadata": {},
   "source": [
    "Implementing an SVM with a polynomial kernel in Python using Scikit-learn is quite straightforward. Scikit-learn provides the SVC class for Support Vector Classification, which allows us to specify different kernel functions, including the polynomial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e7ff2d-2f4c-490f-912f-8448f668a785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "# Set the 'degree' parameter to control the degree of the polynomial\n",
    "# Set the 'C' parameter for regularization (smaller values for more regularization)\n",
    "svm_classifier = SVC(kernel='poly', degree=3, C=1.0)\n",
    "\n",
    "# Train the SVM classifier on the training set\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model on the testing set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy\" ,accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21249bff-2057-42d8-b18f-57fb0b9357cc",
   "metadata": {},
   "source": [
    "# Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc49ceec-4e5d-4532-9cf9-c4539a211b47",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), epsilon (ε) is a parameter that controls the width of the margin around the predicted values. It determines the zone within which errors are not penalized, and data points within this zone are considered to be correctly predicted (i.e., they do not contribute to the loss function).\n",
    "\n",
    "Increasing the value of epsilon in SVR has an impact on the number of support vectors in the model. Support vectors are the data points that lie on the margin or within the margin zone and influence the construction of the regression function.\n",
    "\n",
    "Here's how increasing the value of epsilon affects the number of support vectors:\n",
    "\n",
    "Larger Epsilon (Wider Margin Zone):\n",
    "\n",
    "When epsilon is large, the margin zone becomes wider, allowing more data points to fall within this zone without incurring a penalty.\n",
    "A wider margin zone means that data points can have larger deviations from the predicted values without affecting the model's performance significantly.\n",
    "As a result, more data points may fall within the margin zone and become support vectors.\n",
    "Increasing epsilon might lead to an increase in the number of support vectors.\n",
    "\n",
    "\n",
    "Smaller Epsilon (Narrower Margin Zone):\n",
    "\n",
    "Conversely, when epsilon is small, the margin zone becomes narrower, and the model becomes more sensitive to deviations from the predicted values.\n",
    "Fewer data points will be allowed to fall within the narrow margin zone without being penalized.\n",
    "As a consequence, fewer data points may become support vectors.\n",
    "Decreasing epsilon might lead to a decrease in the number of support vectors.\n",
    "In summary, the value of epsilon in SVR controls the width of the margin zone around the predicted values. A larger epsilon widens the margin zone, allowing more data points to fall within it without being treated as errors. Consequently, more data points may become support vectors. On the other hand, a smaller epsilon narrows the margin zone, making the model more sensitive to deviations, and fewer data points may become support vectors. The choice of epsilon should be made based on the problem at hand and the desired trade-off between model complexity and sensitivity to errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e3a343-a799-4e02-bc11-7b76a7b3171d",
   "metadata": {},
   "source": [
    "# Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398aae78-7713-438e-920b-98271a5878cb",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), the choice of kernel function, C parameter, epsilon parameter, and gamma parameter significantly impact the model's performance and generalization ability. Let's discuss each parameter and its effects:\n",
    "\n",
    "Kernel Function:\n",
    "\n",
    "The kernel function determines how the data points are mapped into a higher-dimensional feature space to find nonlinear relationships between features and target variables.\n",
    "Different kernel functions (e.g., linear, polynomial, radial basis function) have varying effects on the model's flexibility and ability to capture complex patterns.\n",
    "For example, the radial basis function (RBF) kernel is more flexible and can fit complex data distributions, while the linear kernel is more suitable for linearly separable data.\n",
    "Choose the kernel function based on the nature of the data and the complexity of the underlying relationship between features and the target variable.\n",
    "\n",
    "\n",
    "C Parameter:\n",
    "\n",
    "The C parameter controls the trade-off between maximizing the margin and minimizing the training errors.\n",
    "A smaller C value introduces more regularization, leading to a wider margin and more tolerance for errors. This may result in a simpler model that may underfit the data.\n",
    "A larger C value reduces regularization, leading to a narrower margin and stricter error tolerance. This may lead to a more complex model that may overfit the data.\n",
    "Increase C when you want the model to focus on individual data points and reduce the regularization. Decrease C when you want the model to be less sensitive to individual data points and prioritize a wider margin.\n",
    "\n",
    "\n",
    "Epsilon Parameter (ε):\n",
    "\n",
    "The epsilon parameter defines the width of the margin zone in the ε-insensitive loss function. Data points within this zone are not considered errors and do not contribute to the loss function.\n",
    "A larger ε value increases the width of the margin zone, making the model more tolerant to deviations from the predicted values.\n",
    "A smaller ε value narrows the margin zone, making the model more sensitive to errors and deviations.\n",
    "Increase ε when you expect some level of noise in the target variable and want the model to be less sensitive to small deviations. Decrease ε when you want the model to be more sensitive to errors and outliers.\n",
    "\n",
    "\n",
    "Gamma Parameter:\n",
    "\n",
    "The gamma parameter is specific to certain kernel functions, such as RBF, sigmoid, and polynomial kernels.\n",
    "It defines the influence of a single training example on the decision boundary.\n",
    "A smaller gamma value creates a broader and smoother decision boundary, making the model more generalizable but potentially less accurate.\n",
    "A larger gamma value creates a more complex and wiggly decision boundary, leading to potential overfitting on the training data.\n",
    "Increase gamma when you have confidence in your training data and want the model to have a tight decision boundary. Decrease gamma when you want a smoother and more generalized decision boundary.\n",
    "It's essential to tune these parameters using techniques like cross-validation to find the best combination that yields a model with good performance on unseen data. The choice of these parameters should be based on the data characteristics, the problem complexity, and the trade-off between model complexity and generalization. Avoid overfitting by choosing appropriate regularization and kernel parameters and validate the model's performance on a separate testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6c647a-7e88-49a4-b201-a4e84bee2fba",
   "metadata": {},
   "source": [
    "# Q5. Assignment:\n",
    "L Import the necessary libraries and load the dataseg\n",
    "\n",
    "L Split the dataset into training and testing setZ\n",
    "\n",
    "L Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "\n",
    "L Create an instance of the SVC classifier and train it on the training datW\n",
    "\n",
    "L hse the trained classifier to predict the labels of the testing datW\n",
    "\n",
    "L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK\n",
    "\n",
    "L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "improve its performanc_\n",
    "\n",
    "L Train the tuned classifier on the entire dataseg\n",
    "\n",
    "L Save the trained classifier to a file for future use.\n",
    "\n",
    "You can use any dataset of your choice for this assignment, but make sure it is suitable for\n",
    "classification and has a sufficient number of features and samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a9e5505-d8b1-4663-adf5-386cee01ad81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "Best Parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Best Estimator: SVC(C=10, kernel='linear')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=10, kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=10, kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=10, kernel='linear')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "df = load_iris()\n",
    "X = df.data\n",
    "y = df.target\n",
    "\n",
    "# Step 3: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 4: Preprocess the data (in this example, we will use StandardScaler for scaling)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_scaled = scaler.transform(X)  # Preprocess the entire dataset\n",
    "\n",
    "# Step 5: Create an instance of the SVC classifier and train it on the training data\n",
    "svc_classifier = SVC()\n",
    "svc_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 6: Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Step 7: Evaluate the performance of the classifier using accuracy as the metric\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# You can also use classification_report to get more detailed performance metrics\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 8: Tune the hyperparameters of the SVC classifier using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=SVC(), param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and the best estimator from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "best_svc_classifier = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Estimator:\", best_svc_classifier)\n",
    "\n",
    "# Step 9: Train the tuned classifier on the entire dataset\n",
    "best_svc_classifier.fit(X_scaled, y)  # Use X_scaled instead of X_test_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89db90a8-5b3d-40c6-b45e-fe04db37ca8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
