{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eadbb948-7473-40d6-ac73-85c28a7a2843",
   "metadata": {},
   "source": [
    "# Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a8760e-028e-46bd-9965-79d1b6958655",
   "metadata": {},
   "source": [
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use Bayes' theorem.\n",
    "\n",
    "Let's define the events:\n",
    "- A: An employee uses the company's health insurance plan.\n",
    "- B: An employee is a smoker.\n",
    "\n",
    "We are given the following probabilities:\n",
    "- \\( P(A) \\): Probability that an employee uses the health insurance plan = 0.70 (70%).\n",
    "- \\( P(B|A) \\): Probability that an employee is a smoker given that they use the health insurance plan = 0.40 (40%).\n",
    "\n",
    "Now, we want to find \\( P(B|A) \\), the probability that an employee is a smoker given that they use the health insurance plan. This can be calculated using Bayes' theorem:\n",
    "\n",
    "\n",
    "P(B∣A)= P(A∣B)⋅P(B)/P(A)\n",
    "\n",
    "Since we are not given the probability ( P(B)) directly, we can calculate it using the law of total probability:\n",
    "\n",
    " P(B) = P(B|A). P(A) + P(B-A). P(- A) \n",
    "\n",
    "where (- A ) represents the event that an employee does not use the health insurance plan.\n",
    "\n",
    "Since we know that ( P(A) = 0.70 ) and ( P(B|A) = 0.40 ), we can find ( P(B) ) as follows:\n",
    "\\[ P(B) = 0.40 . 0.70 + P(B|- A). (1 - 0.70)\n",
    "\n",
    "Now, we need one more piece of information: the proportion of employees who do not use the health insurance plan ( P(- A)), which can be calculated as:\n",
    " P(- A) = 1 - P(A) = 1 - 0.70 = 0.30 \n",
    "\n",
    "With this information, we can find ( P(B)\n",
    "P(B) = 0.40 .0.70 + P(B|- A) . 0.30\n",
    "\n",
    "Now we can calculate  P(B|A) using Bayes' theorem:\n",
    " P(B|A) = P(A|B). P(B)/ P(A)\n",
    "\n",
    "Given that  P(A|B) = P(B|A) = 0.40 (this information is given in the problem), we can now find  P(B|A)\n",
    "P(B|A) = 0.40. 0.40 /0.70\n",
    "\n",
    "Calculate the value:\n",
    "P(B|A) = 0.16/0.70 \n",
    "approx =0.229 \n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.229 or 22.9%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43605530-c21f-4d59-b3b7-73f26ac26a5e",
   "metadata": {},
   "source": [
    "# Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bc18f2-638e-4767-92f6-83a4d0ecc061",
   "metadata": {},
   "source": [
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of data they are best suited for and how they handle features.\n",
    "\n",
    "Bernoulli Naive Bayes:\n",
    "Best suited for binary data, where each feature can take on only two values: 0 or 1 (representing the absence or presence of a particular feature).\n",
    "It assumes that each feature is binary and independent of each other, meaning the presence of one feature does not affect the presence of another.\n",
    "Typically used for text classification tasks where the features represent the presence or absence of words in a document or message.\n",
    "\n",
    "Multinomial Naive Bayes:\n",
    "Suited for discrete count data, where each feature represents the count or frequency of a specific event.\n",
    "It is commonly used for text classification tasks where features are word frequencies or term counts.\n",
    "Unlike Bernoulli Naive Bayes, Multinomial Naive Bayes allows for features with multiple discrete values, not just binary.\n",
    "Both Bernoulli and Multinomial Naive Bayes are variants of the Naive Bayes algorithm. They are based on the same underlying principles but make different assumptions about the nature of the data.\n",
    "\n",
    "In summary, choose Bernoulli Naive Bayes when dealing with binary data (presence/absence), and choose Multinomial Naive Bayes when dealing with count data (word frequencies, term counts) or other discrete data with multiple values. The choice of which variant to use depends on the specific characteristics of your data and the nature of the classification problem you are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf16ed7-9619-4573-bd5f-d97956086a08",
   "metadata": {},
   "source": [
    "# Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1734191-9399-4bbe-bc46-fe2d4d08c88f",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes handles missing values by ignoring them during the probability calculations. When encountering a missing value for a particular feature, the algorithm simply excludes that feature from the probability calculations for the corresponding class.\n",
    "\n",
    "Here's how Bernoulli Naive Bayes handles missing values step-by-step:\n",
    "\n",
    "Training Phase:\n",
    "\n",
    "During the training phase, the algorithm calculates probabilities for each class based on the presence or absence of features in the training data.\n",
    "If a feature is missing for a specific instance in the training data, the algorithm treats it as if the feature is not present (i.e., it assumes the value is 0).\n",
    "The algorithm calculates the probabilities of each class based on the presence or absence of each feature in the training data, including the missing values treated as 0.\n",
    "Testing Phase:\n",
    "\n",
    "During the testing phase, when the algorithm encounters an instance with missing values, it still uses the same probabilities calculated during training.\n",
    "For each missing feature in the instance, the algorithm ignores that feature during the probability calculations for each class.\n",
    "The missing feature is treated as if it were not present, so its absence contributes to the probability calculations, just like during training.\n",
    "By treating missing values as the absence of features, Bernoulli Naive Bayes effectively ignores the missing values and continues with the classification process. This approach simplifies the implementation and is particularly useful when dealing with sparse binary data, as is common in text classification tasks.\n",
    "\n",
    "However, it's important to note that the handling of missing values in Bernoulli Naive Bayes might not be optimal in all cases. In some scenarios, a more sophisticated approach, such as imputation or considering missing values as a separate category, may be more appropriate to handle missing data effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdf6edd-b660-460b-99e0-8c24b9b0db27",
   "metadata": {},
   "source": [
    "# Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1825be71-7e20-4cf1-aa6e-91291bb60c0b",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is one of the variants of the Naive Bayes algorithm and is suitable for handling continuous or numeric features. It assumes that the features follow a Gaussian (normal) distribution within each class.\n",
    "\n",
    "For multi-class classification, where the target variable can take on more than two distinct classes, Gaussian Naive Bayes extends its capabilities to handle multiple classes. It does this by estimating the parameters (mean and variance) of the Gaussian distribution for each feature within each class.\n",
    "\n",
    "Here's how Gaussian Naive Bayes works for multi-class classification:\n",
    "\n",
    "Training Phase:\n",
    "\n",
    "During the training phase, the algorithm calculates the mean and variance of each feature for each class using the training data.\n",
    "For each class, it calculates the mean and variance of each feature based on the instances belonging to that class.\n",
    "These mean and variance values are used to model the Gaussian distribution for each feature within each class.\n",
    "\n",
    "Testing Phase:\n",
    "\n",
    "During the testing phase, when the algorithm encounters a new instance with its feature values, it calculates the probability of the instance belonging to each class using the Gaussian probability density function (PDF) for each feature.\n",
    "It applies Bayes' theorem to find the conditional probability of the instance belonging to each class given its feature values.\n",
    "The class with the highest conditional probability is predicted as the output class for the instance.\n",
    "Gaussian Naive Bayes is a powerful and computationally efficient algorithm for multi-class classification, especially when dealing with continuous-valued features that follow a Gaussian distribution. However, it assumes that the features are independent of each other given the class, which may not always hold true in real-world scenarios. Nonetheless, Gaussian Naive Bayes can still perform well in many multi-class classification tasks, especially when the independence assumption approximately holds or when there is a large amount of data available for training.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5d17d4-8dfb-4379-b9ce-896ef21e4760",
   "metadata": {},
   "source": [
    "# Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "702b5f12-9eb4-4ccb-b86e-2bb8d52cb272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "data = fetch_openml(name='spambase')\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Next, we can create instances of the Bernoulli Naive Bayes, Multinomial Naive Bayes,\n",
    "# and Gaussian Naive Bayes classifiers and fit them to the training data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e03f78-564f-4e21-a194-0b1bbe318895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "# Bernoulli Naive Bayes\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "#To evaluate the performance of each classifier using 10-fold cross-validation, \n",
    "#we can use the cross_val_score function from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd7050ba-9285-4d5e-80f5-0614abaf4f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.8839380364047911\n",
      "Precision:\n",
      "Recall:\n",
      "F1 score:\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.7863496180326323\n",
      "Precision:\n",
      "Recall:\n",
      "F1 score:\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy: 0.8217730830896915\n",
      "Precision:\n",
      "Recall:\n",
      "F1 score:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall:\u001b[39m\u001b[38;5;124m\"\u001b[39m, )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1 score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, )\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, \u001b[43my_pred\u001b[49m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Bernoulli Naive Bayes\n",
    "bnb_scores = cross_val_score(bnb, X, y, cv=10)\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(\"Accuracy:\", bnb_scores.mean())\n",
    "print(\"Precision:\", )\n",
    "print(\"Recall:\", )\n",
    "print(\"F1 score:\", )\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "mnb_scores = cross_val_score(mnb, X, y, cv=10)\n",
    "print(\"Multinomial Naive Bayes:\")\n",
    "print(\"Accuracy:\", mnb_scores.mean())\n",
    "print(\"Precision:\", )\n",
    "print(\"Recall:\", )\n",
    "print(\"F1 score:\", )\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "gnb_scores = cross_val_score(gnb, X, y, cv=10)\n",
    "print(\"Gaussian Naive Bayes:\")\n",
    "print(\"Accuracy:\", gnb_scores.mean())\n",
    "print(\"Precision:\", )\n",
    "print(\"Recall:\", )\n",
    "print(\"F1 score:\", )\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
