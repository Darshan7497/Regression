{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "947cae28-24b4-4e02-8904-8670529773a8",
   "metadata": {},
   "source": [
    "# Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b116ccf-955e-4220-b359-bcdb3b4d9f48",
   "metadata": {},
   "source": [
    "Bayes' theorem, also known as Bayes' rule or Bayes' law, is a fundamental theorem in probability theory and statistics. It provides a way to update our beliefs or probabilities about an event based on new evidence or observations.\n",
    "\n",
    "The theorem is named after the Reverend Thomas Bayes, who first formulated the idea, though it was published posthumously in the 18th century by another mathematician, Richard Price.\n",
    "\n",
    "Mathematically, Bayes' theorem can be stated as follows:\n",
    "\n",
    "\n",
    "P(A∣B)= \n",
    "P(B∣A)⋅P(A)/\n",
    "P(B)\n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "\n",
    "P(A∣B) is the conditional probability of event A occurring given that event B has occurred.\n",
    "\n",
    "P(B∣A) is the conditional probability of event B occurring given that event A has occurred.\n",
    "\n",
    "P(A) is the prior probability or the initial probability of event A occurring before any new evidence is considered.\n",
    "\n",
    "P(B) is the prior probability or the initial probability of event B occurring before any new evidence is considered.\n",
    "In words, Bayes' theorem tells us how to update our prior beliefs (prior probability) about event A when we have new evidence (event B). It helps us find the probability of A occurring given that B has occurred, by combining the prior probability of A, the likelihood of B given A, and the overall probability of B.\n",
    "\n",
    "Bayes' theorem has applications in various fields, including machine learning, statistics, medical diagnosis, natural language processing, and more. It plays a crucial role in Bayesian statistics, where it provides a foundation for updating probabilities as new data is observed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce3c271-290d-4e44-a51e-11f98d2451a5",
   "metadata": {},
   "source": [
    "# Q2. What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf70ad7-2372-47a1-b592-ec42d2afa889",
   "metadata": {},
   "source": [
    "Bayes' theorem can be stated as follows:\n",
    "\n",
    "P(A∣B)= P(B∣A)⋅P(A)/ P(B)​"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da1642c-0468-410d-913f-c4a75e297a10",
   "metadata": {},
   "source": [
    "# Q3. How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a006d519-496c-44e2-b53f-8d4ced045648",
   "metadata": {},
   "source": [
    "Bayes' theorem is used in various practical applications across different fields due to its ability to update probabilities based on new evidence. Here are some common uses of Bayes' theorem in practice:\n",
    "\n",
    "Medical Diagnosis: Bayes' theorem is used in medical diagnosis to update the probability of a disease or condition given the results of diagnostic tests. It allows doctors to incorporate test results and patient information to refine their diagnosis.\n",
    "\n",
    "Spam Filtering: In email spam filtering, Bayes' theorem is applied to calculate the probability that an incoming email is spam given certain characteristics (e.g., words, phrases, or patterns) in the email content.\n",
    "\n",
    "Machine Learning and Bayesian Statistics: Bayes' theorem forms the foundation for Bayesian statistics, which provides a framework for updating probabilities as new data is observed. Bayesian methods are used in various machine learning applications, including text classification, recommendation systems, and natural language processing.\n",
    "\n",
    "Fault Diagnosis: Bayes' theorem is employed in fault diagnosis and reliability analysis to update the probability of system failures or malfunctions based on observed data from sensors or other diagnostic sources.\n",
    "\n",
    "Document Classification: Bayes' theorem is used in document classification tasks, such as categorizing documents into different topics or classes based on word frequencies or features.\n",
    "\n",
    "Weather Forecasting: In meteorology, Bayes' theorem is applied to update weather predictions based on new observations and sensor data.\n",
    "\n",
    "Financial Modeling: Bayes' theorem is used in financial modeling for risk assessment, fraud detection, and portfolio management, where probabilities need to be updated with new market data.\n",
    "\n",
    "A/B Testing: Bayes' theorem is applied in A/B testing to analyze the results of experiments and determine the effectiveness of different variations.\n",
    "\n",
    "Language Translation: In natural language processing and machine translation, Bayes' theorem helps update the probabilities of word sequences given observed translations.\n",
    "\n",
    "In all these practical applications, Bayes' theorem provides a systematic way to combine prior knowledge or beliefs with new evidence to make more informed decisions and predictions. By updating probabilities based on observed data, Bayes' theorem allows for better reasoning and decision-making in uncertain situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41e6846-2fee-4586-8d0d-8fa82fe70e29",
   "metadata": {},
   "source": [
    "# Q4. What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fbb854-00d7-4965-9eea-0159fe813298",
   "metadata": {},
   "source": [
    "Bayes' theorem is directly related to conditional probability. In fact, Bayes' theorem is a mathematical formula that allows us to calculate conditional probabilities in situations where we have some prior knowledge or beliefs.\n",
    "\n",
    "P(A∣B)= P(B∣A)⋅P(A)/ P(B)\n",
    "\n",
    "Here, \n",
    "P(A∣B) represents the conditional probability of event A occurring given that event B has occurred. This means we want to find the probability of A happening, taking into account the occurrence of B.\n",
    "\n",
    "Now, let's look at the components of Bayes' theorem in terms of conditional probability:\n",
    "\n",
    "P(B∣A): This is the conditional probability of event B occurring given that event A has occurred. It represents the likelihood of observing B under the assumption that A is true.\n",
    "\n",
    "P(A): This is the prior probability or the initial probability of event A occurring before any new evidence (event B) is considered. It represents our initial beliefs or knowledge about the likelihood of A.\n",
    "P(B): This is the prior probability or the initial probability of event B occurring before any new evidence (event A) is considered. It represents our initial beliefs or knowledge about the likelihood of B.\n",
    "\n",
    "So, Bayes' theorem combines these components to update our prior beliefs (prior probabilities) about event A based on new evidence (event B) to obtain the conditional probability \n",
    "P(A∣B).\n",
    "\n",
    "In summary, Bayes' theorem and conditional probability are closely related because Bayes' theorem allows us to calculate conditional probabilities by incorporating prior knowledge and new evidence. It provides a systematic way to update our probabilities based on observed data or information, leading to more accurate and informed probabilistic reasoning in various applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfc4bca-b1ef-4503-8077-eb71efc50c87",
   "metadata": {},
   "source": [
    "# Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a0283-e1c2-4326-97bd-c99f5786df8d",
   "metadata": {},
   "source": [
    "Choosing the appropriate type of Naive Bayes classifier depends on the nature of the features and the distribution of the data in your specific problem. Each variant of Naive Bayes makes different assumptions about the data, and selecting the right one can lead to better performance and more accurate predictions. Here are some guidelines to help you choose the appropriate type of Naive Bayes classifier for your problem:\n",
    "\n",
    "Gaussian Naive Bayes (GNB):\n",
    "Use GNB when dealing with continuous or real-valued features that approximately follow a Gaussian (normal) distribution.\n",
    "GNB is suitable for problems where the features are numeric and have a continuous range of values.\n",
    "Examples: Problems involving sensor readings, measurements, or any continuous variables.\n",
    "\n",
    "Multinomial Naive Bayes (MNB):\n",
    "Use MNB when dealing with discrete count data, such as word frequencies, term counts, or any non-negative integers.\n",
    "MNB is commonly used for text classification tasks, where features represent word occurrences or frequencies.\n",
    "Examples: Text classification (spam vs. non-spam emails, sentiment analysis, topic categorization).\n",
    "\n",
    "Bernoulli Naive Bayes (BNB):\n",
    "Use BNB when dealing with binary features where each feature is a Boolean variable representing the presence or absence of a specific attribute or word.\n",
    "BNB is appropriate for problems involving binary data, where features are either 0 or 1.\n",
    "Examples: Document classification with binary features (spam vs. non-spam, relevant vs. irrelevant).\n",
    "Choosing the right type of Naive Bayes classifier is essential for achieving good performance. However, it's important to remember that the \"naive\" assumption of feature independence in Naive Bayes may or may not hold in practice. In some cases, the features might not be truly independent, but Naive Bayes can still work surprisingly well, especially when the chosen variant aligns well with the data characteristics.\n",
    "\n",
    "It's a good practice to try different variants and evaluate their performance using techniques like cross-validation to determine which one performs best for your specific problem. Additionally, feature engineering and data preprocessing can also influence the choice of the Naive Bayes variant and overall model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4063cca7-273f-4575-a929-a722a63a501a",
   "metadata": {},
   "source": [
    "# Q6. Assignment:\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
    "each feature value for each class:\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "A 3 3 4 4 3 3 3\n",
    "B 2 2 1 2 2 2 3\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
    "to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3ca434-351d-46d2-bd61-1b889c2357c4",
   "metadata": {},
   "source": [
    "To classify the new instance with features X1 = 3 and X2 = 4 using Naive Bayes, we need to calculate the posterior probability of each class given the feature values. We can use the Naive Bayes formula:\n",
    "\n",
    "P(A|X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) / P(X1=3,X2=4)\n",
    "P(B|X1=3,X2=4) = P(X1=3,X2=4|B) * P(B) / P(X1=3,X2=4)\n",
    "\n",
    "Since the prior probabilities are equal, we can ignore them and focus on the likelihoods:\n",
    "\n",
    "P(X1=3,X2=4|A) = P(X1=3|A) * P(X2=4|A) = 4/13 * 3/13 = 0.090\n",
    "P(X1=3,X2=4|B) = P(X1=3|B) * P(X2=4|B) = 1/7 * 3/7 = 0.061\n",
    "\n",
    "To calculate the evidence probability P(X1=3,X2=4), we need to sum the product of each likelihood and prior probability for each class:\n",
    "\n",
    "P(X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) + P(X1=3,X2=4|B) * P(B) = 0.090 * 0.5 + 0.061 * 0.5 = 0.0755\n",
    "\n",
    "Finally, we can calculate the posterior probabilities using Bayes' theorem:\n",
    "\n",
    "P(A|X1=3,X2=4) = 0.090 * 0.5 / 0.0755 = 0.597\n",
    "P(B|X1=3,X2=4) = 0.061 * 0.5 / 0.0755 = 0.403\n",
    "\n",
    "Therefore, Naive Bayes would predict the new instance to belong to class A, since it has the higher posterior probability of 0.597."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
