{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdd3471d-987a-4b13-8932-19b85f5aca9b",
   "metadata": {},
   "source": [
    "# Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5817de5-805a-473a-9c16-bc6e5b1c19be",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks, which involve predicting a continuous numeric value. It's an ensemble learning method that combines the predictions of multiple weak learners (typically decision trees) to create a stronger overall predictive model.\n",
    "\n",
    "Here's how Gradient Boosting Regression works:\n",
    "\n",
    "Initialization: The algorithm starts with an initial prediction for all data points. This could be a simple measure like the mean of the target values.\n",
    "\n",
    "Calculate Residuals: The difference between the actual target values and the initial predictions are calculated. These differences are called residuals.\n",
    "\n",
    "Build Weak Learner: A weak learner (usually a shallow decision tree) is trained to predict the residuals. The goal is to capture the patterns in the data that the current model is not able to predict accurately.\n",
    "\n",
    "Weighted Combination: The predictions of the weak learner are multiplied by a small learning rate (usually between 0.01 and 0.3) and added to the current model's predictions. This combination helps to update the model.\n",
    "\n",
    "Update Residuals: The residuals are updated by subtracting the predictions made by the weak learner, which reduces the errors in the predictions.\n",
    "\n",
    "Iterative Process: Steps 3 to 5 are repeated iteratively. At each iteration, a new weak learner is trained to predict the updated residuals, and its predictions are combined with the previous model's predictions.\n",
    "\n",
    "Final Prediction: The final prediction is the cumulative sum of the predictions from all the weak learners.\n",
    "\n",
    "Gradient Boosting Regression is robust and generally performs well in practice. It's important to note that increasing the number of iterations (boosting rounds) might lead to overfitting, so careful tuning of hyperparameters like the learning rate and the number of iterations is essential.\n",
    "\n",
    "In summary, Gradient Boosting Regression is an ensemble learning technique that builds a strong regression model by iteratively improving the predictions through a combination of weak learners and adjusted residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72246f3-1022-4134-8422-076cfa8fee58",
   "metadata": {},
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use asimple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd00ebe4-bed5-4173-865a-7150a5af2769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.223805491759683\n",
      "R-squared: 0.3304544699463968\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate some synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X.squeeze() + np.random.normal(0, 0.1, 100)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the prediction with mean of the target\n",
    "y_pred = np.mean(y_train) * np.ones_like(y_train)\n",
    "\n",
    "# Number of boosting rounds\n",
    "n_rounds = 100\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Gradient Boosting algorithm\n",
    "for _ in range(n_rounds):\n",
    "    # Calculate residuals\n",
    "    residuals = y_train - y_pred\n",
    "    \n",
    "    # Fit a weak learner (decision stump) to residuals\n",
    "    tree_pred = np.random.choice(X_train.squeeze(), len(X_train))\n",
    "    tree_residuals = residuals - tree_pred\n",
    "    \n",
    "    # Update predictions using learning rate and tree's predictions\n",
    "    y_pred += learning_rate * tree_residuals\n",
    "    \n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "r2 = r2_score(y_train, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d03d19-3871-4869-b965-4f29d4eb5ef4",
   "metadata": {},
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30695db1-0f2b-4197-82d9-f3f058ba1675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define the hyperparameters and their possible values\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Create a gradient boosting regressor model\n",
    "gb_model = GradientBoostingRegressor()\n",
    "\n",
    "# Use GridSearchCV to search for the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and the corresponding model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "mse = mean_squared_error(y_test, best_model.predict(X_test))\n",
    "r2 = r2_score(y_test, best_model.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "641628a6-e8d3-4db0-a43f-cdcf3a7025d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.007080931044754711\n",
      "R2 0.9810906059004219\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE\" , mse)\n",
    "print(\"R2\" , r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a7eeb-ab3a-4bd3-87b2-abe39cd8991e",
   "metadata": {},
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d057850f-fa34-4d9f-adab-dd75e354dbee",
   "metadata": {},
   "source": [
    "A weak learner in the context of gradient boosting is a simple model that performs slightly better than random chance on a given task. Weak learners are typically models that have limited complexity and predictive power, such as shallow decision trees or linear models.\n",
    "\n",
    "In the gradient boosting process, weak learners are iteratively added to the ensemble. Each weak learner is trained to correct the mistakes made by the previous learners in the ensemble. By combining the predictions of multiple weak learners, gradient boosting creates a strong overall model that can make accurate predictions.\n",
    "\n",
    "Weak learners are \"weak\" in the sense that they individually might not perform well on their own, but when combined through boosting, they contribute to building a robust and accurate predictive model. The key idea of gradient boosting is to fit each new weak learner to the residual errors of the combined ensemble of previous weak learners.\n",
    "\n",
    "The iterative nature of gradient boosting, where each new weak learner corrects the errors of the previous ones, leads to a progressive improvement in the model's predictive performance. This process continues until a predefined number of weak learners are included or until a certain level of performance is achieved.\n",
    "\n",
    "In summary, a weak learner is a simple, often low-complexity model used in the ensemble-building process of gradient boosting. While individual weak learners might not be highly accurate, their collective contribution enhances the overall predictive power of the gradient boosting model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d146857a-113e-4970-b973-4b150964dd9a",
   "metadata": {},
   "source": [
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2089c935-6ef2-445c-9b09-9484faa46bf5",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be understood through the metaphor of a team of experts trying to solve a complex problem.\n",
    "\n",
    "Imagine you have a team of experts, each of whom is good at solving a specific part of the problem but not the entire problem. Individually, these experts might not provide the best solution, but by combining their expertise, the team can collectively arrive at a strong solution.\n",
    "\n",
    "Similarly, in Gradient Boosting, the algorithm builds a predictive model by assembling a sequence of weak models (typically decision trees), each of which addresses a specific aspect of the prediction problem. Initially, the model starts with a simple weak learner that might perform only slightly better than random chance.\n",
    "\n",
    "In each iteration, the algorithm focuses on the mistakes made by the previous model and constructs a new weak learner to correct those mistakes. It calculates the residuals (differences between actual and predicted values) and trains a new weak model to predict these residuals. This new model learns to address the shortcomings of the previous model.\n",
    "\n",
    "The predictions from these weak models are then combined to create an ensemble model. By giving more weight to the predictions of the models that perform well on the remaining errors, the ensemble gradually converges towards a better solution. This process of iteratively improving predictions and learning from mistakes continues until a stopping criterion is met (e.g., a predefined number of iterations or a certain level of accuracy is achieved).\n",
    "\n",
    "The \"gradient\" in Gradient Boosting refers to the technique of using the gradient of the loss function to determine how much weight should be given to the predictions of each weak learner. By adjusting the weights of the weak learners' predictions, the algorithm guides the model towards minimizing the loss function, which ultimately leads to a more accurate predictive model.\n",
    "\n",
    "In summary, the intuition behind Gradient Boosting involves building an ensemble of weak models that collaboratively improve predictions by addressing the errors of the previous models. This process, guided by the gradient of the loss function, gradually leads to a strong predictive model that can effectively handle complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c672646-bc67-47ae-a90b-0d2a5ada4dad",
   "metadata": {},
   "source": [
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59c8972-1e4f-47e3-99ea-c68299b5805c",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners in an iterative and sequential manner. Here's how it works:\n",
    "\n",
    "Initialization: The process begins by initializing the ensemble with a simple model, often a decision tree with a small depth, called a \"weak learner.\" This initial weak learner might only capture a basic understanding of the data, and its predictions might not be very accurate.\n",
    "\n",
    "Predictions and Residuals: The weak learner's predictions are compared to the actual target values. The differences between these predictions and the actual values, known as residuals, are calculated. These residuals represent the errors made by the current weak learner.\n",
    "\n",
    "Creating a New Weak Learner: The next step is to build a new weak learner that focuses on correcting the errors made by the previous model. This new weak learner is trained on the residuals calculated in the previous step. Its goal is to predict these residuals as accurately as possible.\n",
    "\n",
    "Updating the Ensemble's Predictions: The predictions from this new weak learner are scaled by a factor (learning rate) and added to the ensemble's predictions. This update process is performed in a way that gives more weight to the predictions of the new model if they contribute to reducing the overall error.\n",
    "\n",
    "Iterative Improvement: Steps 2-4 are repeated for a predefined number of iterations or until a specific level of accuracy is achieved. In each iteration, a new weak learner is added to the ensemble, and the predictions are updated based on the residuals from the previous iteration.\n",
    "\n",
    "Final Ensemble Prediction: The final prediction of the Gradient Boosting model is the cumulative sum of the predictions from all the weak learners in the ensemble.\n",
    "\n",
    "The key idea behind the Gradient Boosting algorithm is that each new weak learner is trained to correct the errors of the previous ones. This process of iteratively updating the ensemble's predictions based on the residuals gradually improves the overall accuracy of the model. The algorithm uses the gradient of the loss function to determine how much weight should be given to each weak learner's prediction, which ensures that subsequent models focus on the remaining errors. This adaptive and iterative process leads to the creation of a strong predictive model capable of handling complex patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c92bc0-0fd2-4fc3-9686-75af14607f68",
   "metadata": {},
   "source": [
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab800c6-3683-47a4-a746-d54934e976ee",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition of the Gradient Boosting algorithm involves several steps that build upon each other:\n",
    "\n",
    "Objective Function: The first step is to define an objective function that represents the error or loss of the current ensemble's predictions compared to the actual target values. This function quantifies the quality of the current model's predictions.\n",
    "\n",
    "Gradient Descent: The Gradient Boosting algorithm utilizes gradient descent to minimize the objective function. Gradient descent is an optimization technique that iteratively updates the model's parameters in the direction that reduces the objective function. In this context, the model's parameters refer to the predictions made by the weak learners.\n",
    "\n",
    "Residual Calculation: The algorithm calculates the negative gradient of the objective function with respect to the current ensemble's predictions. This gradient represents the residuals or errors in the current model's predictions.\n",
    "\n",
    "Weak Learner Training: A new weak learner, typically a decision tree with limited depth, is trained to predict the negative gradient (residuals) calculated in the previous step. The goal of this weak learner is to capture the patterns in the residuals that the current ensemble failed to predict accurately.\n",
    "\n",
    "Learning Rate: The predictions of the new weak learner are scaled by a factor known as the learning rate. This factor controls how much each weak learner's contribution influences the final ensemble's predictions. A lower learning rate can make the algorithm more robust and stable, while a higher learning rate might lead to faster convergence but risks overshooting the optimal solution.\n",
    "\n",
    "Update Ensemble's Predictions: The predictions from the new weak learner, scaled by the learning rate, are added to the ensemble's current predictions. This update step aims to correct the errors made by the current ensemble using the new model.\n",
    "\n",
    "Repeat: Steps 3-6 are repeated for a specified number of iterations or until a certain level of performance is reached.\n",
    "\n",
    "Final Ensemble Prediction: The final prediction of the Gradient Boosting algorithm is the cumulative sum of the predictions from all the weak learners in the ensemble.\n",
    "\n",
    "The mathematical intuition of Gradient Boosting involves iteratively improving the ensemble's predictions by focusing on the errors made by the current model and using new weak learners to correct those errors. The algorithm uses the gradient of the loss function to guide the updates, hence the name \"Gradient\" Boosting. By minimizing the objective function through gradient descent, the algorithm constructs a powerful ensemble that can capture complex patterns in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
